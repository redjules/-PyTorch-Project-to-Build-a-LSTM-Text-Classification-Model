{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10cTGrN236lrUl5MxX14kHrM-GCFs6EWZ",
      "authorship_tag": "ABX9TyN21yUM2aXobay4JIukfdog",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/redjules/-PyTorch-Project-to-Build-a-LSTM-Text-Classification-Model/blob/main/Music_recommeder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install pyspark"
      ],
      "metadata": {
        "id": "HlOy6XOTApYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogs6q8zFA5x1",
        "outputId": "6ac49309-6feb-4b2e-fb6a-5b5b18c8dab4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75rccFFa-5uf",
        "outputId": "2594afe5-e522-44fc-f89e-448c2f083c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the modules"
      ],
      "metadata": {
        "id": "GIH7OFoLBN0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count, desc, col, max\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n"
      ],
      "metadata": {
        "id": "w5cZUicJBQdD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the spark session"
      ],
      "metadata": {
        "id": "MDCnjCMdB1jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"lastfm\").getOrCreate()"
      ],
      "metadata": {
        "id": "pt3hQ25jB3ZJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset"
      ],
      "metadata": {
        "id": "CEPhEvvZCKqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/data analysis/listenings.csv'\n",
        "df_listenings = spark.read.format('csv').option('header',True).option('inferSchema',True).load(file_path)\n",
        "df_listenings.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEO09jH2CMkz",
        "outputId": "56c37e3d-66d4-47d9-9052-a8924880ea9a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------------+---------------+--------------------+\n",
            "|    user_id|         date|               track|         artist|               album|\n",
            "+-----------+-------------+--------------------+---------------+--------------------+\n",
            "|000Silenced|1299680100000|           Price Tag|       Jessie J|         Who You Are|\n",
            "|000Silenced|1299679920000|Price Tag (Acoust...|       Jessie J|           Price Tag|\n",
            "|000Silenced|1299679440000|Be Mine! (Ballad ...|          Robyn|            Be Mine!|\n",
            "|000Silenced|1299679200000|            Acapella|          Kelis|            Acapella|\n",
            "|000Silenced|1299675660000|   I'm Not Invisible|      The Tease|   I'm Not Invisible|\n",
            "|000Silenced|1297511400000|Bounce (Feat NORE...|       MSTRKRFT|         Fist of God|\n",
            "|000Silenced|1294498440000|Don't Stop The Mu...|        Rihanna|Addicted 2 Bassli...|\n",
            "|000Silenced|1292438340000|               ObZen|      Meshuggah|               ObZen|\n",
            "|000Silenced|1292437740000|   Yama's Messengers|         Gojira|The Way of All Flesh|\n",
            "|000Silenced|1292436360000|On the Brink of E...|   Napalm Death|Time Waits For No...|\n",
            "|000Silenced|1292436360000|On the Brink of E...|   Napalm Death|Time Waits For No...|\n",
            "|000Silenced|1292435940000|        In Deference|   Napalm Death|      Smear Campaign|\n",
            "|000Silenced|1292434920000|      Post(?)organic|    Decapitated|Organic Hallucinosis|\n",
            "|000Silenced|1292434560000|        Mind Feeders|   Dom & Roland| No Strings Attached|\n",
            "|000Silenced|1292434320000|Necrosadistic War...|Cannibal Corpse|                Kill|\n",
            "|000Silenced|1292365560000|     Dance All Night|   Dom & Roland|          Chronology|\n",
            "|000Silenced|1292365260000|          Late Night|   Dom & Roland|          Chronology|\n",
            "|000Silenced|1292365020000|          Freak Seen|   Dom & Roland|          Chronology|\n",
            "|000Silenced|1292364720000|Paradrenasite (Hi...|   Dom & Roland|          Chronology|\n",
            "|000Silenced|1292364300000|               Rhino|   Dom & Roland|          Chronology|\n",
            "+-----------+-------------+--------------------+---------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning tables"
      ],
      "metadata": {
        "id": "Jy2Ma9pmDnAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_listenings = df_listenings.drop('date')\n",
        "df_listenings.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCAXakDtDpR3",
        "outputId": "7d60cf95-3803-4170-aee3-a1f506b52659"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+---------------+--------------------+\n",
            "|    user_id|               track|         artist|               album|\n",
            "+-----------+--------------------+---------------+--------------------+\n",
            "|000Silenced|           Price Tag|       Jessie J|         Who You Are|\n",
            "|000Silenced|Price Tag (Acoust...|       Jessie J|           Price Tag|\n",
            "|000Silenced|Be Mine! (Ballad ...|          Robyn|            Be Mine!|\n",
            "|000Silenced|            Acapella|          Kelis|            Acapella|\n",
            "|000Silenced|   I'm Not Invisible|      The Tease|   I'm Not Invisible|\n",
            "|000Silenced|Bounce (Feat NORE...|       MSTRKRFT|         Fist of God|\n",
            "|000Silenced|Don't Stop The Mu...|        Rihanna|Addicted 2 Bassli...|\n",
            "|000Silenced|               ObZen|      Meshuggah|               ObZen|\n",
            "|000Silenced|   Yama's Messengers|         Gojira|The Way of All Flesh|\n",
            "|000Silenced|On the Brink of E...|   Napalm Death|Time Waits For No...|\n",
            "|000Silenced|On the Brink of E...|   Napalm Death|Time Waits For No...|\n",
            "|000Silenced|        In Deference|   Napalm Death|      Smear Campaign|\n",
            "|000Silenced|      Post(?)organic|    Decapitated|Organic Hallucinosis|\n",
            "|000Silenced|        Mind Feeders|   Dom & Roland| No Strings Attached|\n",
            "|000Silenced|Necrosadistic War...|Cannibal Corpse|                Kill|\n",
            "|000Silenced|     Dance All Night|   Dom & Roland|          Chronology|\n",
            "|000Silenced|          Late Night|   Dom & Roland|          Chronology|\n",
            "|000Silenced|          Freak Seen|   Dom & Roland|          Chronology|\n",
            "|000Silenced|Paradrenasite (Hi...|   Dom & Roland|          Chronology|\n",
            "|000Silenced|               Rhino|   Dom & Roland|          Chronology|\n",
            "+-----------+--------------------+---------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_listenings = df_listenings.na.drop()\n",
        "df_listenings.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyGExg1AD0ji",
        "outputId": "ca3337b4-832b-4d9a-dcfb-96792739fb78"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------------+---------------+--------------------+\n",
            "|    user_id|               track|         artist|               album|\n",
            "+-----------+--------------------+---------------+--------------------+\n",
            "|000Silenced|           Price Tag|       Jessie J|         Who You Are|\n",
            "|000Silenced|Price Tag (Acoust...|       Jessie J|           Price Tag|\n",
            "|000Silenced|Be Mine! (Ballad ...|          Robyn|            Be Mine!|\n",
            "|000Silenced|            Acapella|          Kelis|            Acapella|\n",
            "|000Silenced|   I'm Not Invisible|      The Tease|   I'm Not Invisible|\n",
            "|000Silenced|Bounce (Feat NORE...|       MSTRKRFT|         Fist of God|\n",
            "|000Silenced|Don't Stop The Mu...|        Rihanna|Addicted 2 Bassli...|\n",
            "|000Silenced|               ObZen|      Meshuggah|               ObZen|\n",
            "|000Silenced|   Yama's Messengers|         Gojira|The Way of All Flesh|\n",
            "|000Silenced|On the Brink of E...|   Napalm Death|Time Waits For No...|\n",
            "|000Silenced|On the Brink of E...|   Napalm Death|Time Waits For No...|\n",
            "|000Silenced|        In Deference|   Napalm Death|      Smear Campaign|\n",
            "|000Silenced|      Post(?)organic|    Decapitated|Organic Hallucinosis|\n",
            "|000Silenced|        Mind Feeders|   Dom & Roland| No Strings Attached|\n",
            "|000Silenced|Necrosadistic War...|Cannibal Corpse|                Kill|\n",
            "|000Silenced|     Dance All Night|   Dom & Roland|          Chronology|\n",
            "|000Silenced|          Late Night|   Dom & Roland|          Chronology|\n",
            "|000Silenced|          Freak Seen|   Dom & Roland|          Chronology|\n",
            "|000Silenced|Paradrenasite (Hi...|   Dom & Roland|          Chronology|\n",
            "|000Silenced|               Rhino|   Dom & Roland|          Chronology|\n",
            "+-----------+--------------------+---------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_numbers = df_listenings.count()\n",
        "column_numbers = len(df_listenings.columns)\n",
        "print(row_numbers, column_numbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz8egjqSEM9S",
        "outputId": "06e86584-e7ff-4eb3-bc42-14eb29d9d42f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13758905 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's Perform some aggregation\n",
        "\n",
        "to see how many times each user has listened to specific track\n",
        "\n"
      ],
      "metadata": {
        "id": "w8FP_2HSD9Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_listenings_agg = df_listenings.select('user_id','track').groupby('user_id','track').agg(count('*')).orderBy('user_id')\n",
        "\n",
        "df_listenings_agg.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoz3J9gzEAGN",
        "outputId": "a72ea214-7022-4578-d232-f5d289d5a071"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------+\n",
            "|user_id|               track|count(1)|\n",
            "+-------+--------------------+--------+\n",
            "| --Seph| White Winter Hymnal|       3|\n",
            "| --Seph|         The Funeral|       1|\n",
            "| --Seph|Hope There's Someone|       1|\n",
            "| --Seph|         The Painter|       1|\n",
            "| --Seph|          Je te veux|       1|\n",
            "| --Seph|            War Pigs|       1|\n",
            "| --Seph|                 F12|       1|\n",
            "| --Seph|                Team|       1|\n",
            "| --Seph|          Nightmares|       1|\n",
            "| --Seph|               Radio|       1|\n",
            "| --Seph|   All I Want Is You|       1|\n",
            "| --Seph|    Little by Little|       2|\n",
            "| --Seph|        After Nature|       1|\n",
            "| --Seph|In the Hall of th...|       1|\n",
            "| --Seph|   Hey There Delilah|       1|\n",
            "| --Seph|   Let's Call It Off|       1|\n",
            "| --Seph|               Leloo|       1|\n",
            "| --Seph|             Pack Up|       1|\n",
            "| --Seph|           Introitus|       1|\n",
            "| --Seph|        The Leanover|       1|\n",
            "+-------+--------------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_numbers = df_listenings_agg.count()\n",
        "column_numbers = len(df_listenings_agg.columns)\n",
        "print(row_numbers,column_numbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDIzxXdbFbXW",
        "outputId": "32be0d37-a5ce-47c9-d403-d2f34835cf89"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9930128 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_listenings_agg = df_listenings_agg.limit(20000)"
      ],
      "metadata": {
        "id": "x769OlFiFub1"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's convert ohe user id and track columns into unique integers"
      ],
      "metadata": {
        "id": "O1ui6pdPF1Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexer = [StringIndexer(inputCol=col, outputCol=col+'_index').fit(df_listenings_agg) for col in list(set(df_listenings_agg.columns) - set(['count']))]\n",
        "pipeline = Pipeline(stages=indexer)\n",
        "\n",
        "data = pipeline.fit(df_listenings_agg).transform(df_listenings_agg)\n",
        "data.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6J3H8MUF8S3",
        "outputId": "e2ee12c9-ea43-40eb-c11a-a364466f9481"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------+-------------+-----------+--------------+\n",
            "|user_id|               track|count(1)|user_id_index|track_index|count(1)_index|\n",
            "+-------+--------------------+--------+-------------+-----------+--------------+\n",
            "| --Seph|          Nightmares|       1|         69.0|    10600.0|           0.0|\n",
            "| --Seph|Virus (Luke Fair ...|       1|         69.0|    15893.0|           0.0|\n",
            "| --Seph|Airplanes [feat H...|       1|         69.0|      521.0|           0.0|\n",
            "| --Seph|Belina (Original ...|       1|         69.0|     3280.0|           0.0|\n",
            "| --Seph|              Monday|       1|         69.0|      334.0|           0.0|\n",
            "| --Seph|Hungarian Dance No 5|       1|         69.0|     7555.0|           0.0|\n",
            "| --Seph|       Life On Mars?|       1|         69.0|     1164.0|           0.0|\n",
            "| --Seph|  California Waiting|       1|         69.0|      195.0|           0.0|\n",
            "| --Seph|       Phantom Pt II|       1|         69.0|     1378.0|           0.0|\n",
            "| --Seph|   Summa for Strings|       1|         69.0|    13737.0|           0.0|\n",
            "| --Seph|      Hour for magic|       2|         69.0|     7492.0|           1.0|\n",
            "| --Seph|Hungarian Rhapsod...|       1|         69.0|     7556.0|           0.0|\n",
            "| --Seph|     The Way We Were|       1|         69.0|    14958.0|           0.0|\n",
            "| --Seph| Air on the G String|       1|         69.0|     2456.0|           0.0|\n",
            "| --Seph|Vestido Estampado...|       1|         69.0|    15847.0|           0.0|\n",
            "| --Seph|        Window Blues|       1|         69.0|     1841.0|           0.0|\n",
            "| --Seph| White Winter Hymnal|       3|         69.0|       59.0|           2.0|\n",
            "| --Seph|         The Embrace|       1|         69.0|    14386.0|           0.0|\n",
            "| --Seph|          Paris 2004|       7|         69.0|    11311.0|           7.0|\n",
            "| --Seph|Chelsea Hotel - L...|       1|         69.0|     4183.0|           0.0|\n",
            "+-------+--------------------+--------+-------------+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.select('user_id_index','track_index','count(1)').orderBy('user_id_index')"
      ],
      "metadata": {
        "id": "HcOFPJhTHPJt"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test Data"
      ],
      "metadata": {
        "id": "WuSfCVh_Hg7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(training, test) = data.randomSplit([0.5,0.5])"
      ],
      "metadata": {
        "id": "cKon_hdeHjSQ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's Create our Model"
      ],
      "metadata": {
        "id": "ek767kHeIMWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USERID = 'user_id_index'\n",
        "TRACK = 'track_index'\n",
        "COUNT = 'count(1)'\n",
        "\n",
        "als = ALS(maxIter=5, regParam=0.01, userCol=USERID, itemCol=TRACK, ratingCol=COUNT)\n",
        "model = als.fit(training)\n",
        "\n",
        "predictions = model.transform(test)"
      ],
      "metadata": {
        "id": "vZI5Fvk6IPJv",
        "outputId": "db6922b9-1341-4bf9-c2ac-762e57a59361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-89cce2d74ed4>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregParam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSERID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRACK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratingCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1175.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 114.0 failed 1 times, most recent failure: Lost task 0.0 in stage 114.0 (TID 417) (0d9e6e1f6659 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$3902/0x00000008415b1040: (string) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:322)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:320)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Unseen label: Big Time. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:290)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:173)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:167)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:143)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:181)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:181)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:183)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:266)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:264)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:264)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:236)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:381)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:366)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:190)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:189)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3694)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3692)\n\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:727)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:714)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:616)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$3902/0x00000008415b1040: (string) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:322)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:320)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:905)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:905)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: Big Time. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 22 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate top 10 Track recommendations for each user"
      ],
      "metadata": {
        "id": "wSombmh6Jksu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recs = model.recommendForAllUsers(10)"
      ],
      "metadata": {
        "id": "xVgerKozJqqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recs.show()"
      ],
      "metadata": {
        "id": "MUQwaVOVJxdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recs.take(1)"
      ],
      "metadata": {
        "id": "kJGUJNg7PFwo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}